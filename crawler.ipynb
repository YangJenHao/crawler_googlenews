{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "# 基本爬蟲環境設置\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "response = requests.get(\n",
    "    \"https://news.google.com/topstories?hl=zh-TW&gl=TW&ceid=TW:zh-Hant\")\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "# print(soup.prettify())  #輸出排版後的HTML內容 除錯用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找titles\n",
    "newsTitles = []\n",
    "titles = soup.find_all('a', class_='DY5T1d RZIKme')\n",
    "for title in titles:\n",
    "    newsTitles.append(title.string)\n",
    "    # print(title.string) # 查看所有titles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找新聞媒體\n",
    "newsMedia = []\n",
    "medias = soup.find_all('a', class_='wEwyrc AVN2gc uQIVzc Sksgp slhocf')\n",
    "for media in medias:\n",
    "    newsMedia.append(media.string)\n",
    "    # print(media.string) # 查看所有medias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找urls\n",
    "newsUrls = []\n",
    "urls = soup.find_all('a', class_='VDXfz')\n",
    "for url in urls:\n",
    "    url = url.get(\"href\")\n",
    "    newurl = url.replace('.','https://news.google.com',1)\n",
    "    newsUrls.append(newurl)\n",
    "    # print(newurl) # 查看所有urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理成新聞清單\n",
    "newsList = []\n",
    "for i in range(len(newsTitles)):\n",
    "    newsList.append('news' + str(i+1).rjust(4,'0') + '\\t' + newsUrls[i]+ '\\t' + newsMedia[i]+ '\\t' + newsTitles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬到的新聞數量 = 30\n"
     ]
    }
   ],
   "source": [
    "# 存檔成 newslist.txt\n",
    "path = 'newslist.txt'\n",
    "print('爬到的新聞數量 =', len(newsList))\n",
    "with open(path, 'w') as f:\n",
    "    for i in range(len(newsList)):\n",
    "        if i == len(newsList) -1:\n",
    "            f.write(newsList[i])\n",
    "        else:\n",
    "            f.write(newsList[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/m9/207x19992g1gd9041gpf1c8w0000gn/T/jieba.cache\n",
      "Loading model cost 0.471 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "編號news0001已爬蟲完成！\n",
      "編號news0002已爬蟲完成！\n",
      "編號news0003已爬蟲完成！\n",
      "編號news0004已爬蟲完成！\n",
      "編號news0005已爬蟲完成！\n",
      "編號news0006已爬蟲完成！\n",
      "編號news0007已爬蟲完成！\n",
      "編號news0008已爬蟲完成！\n",
      "編號news0009已爬蟲完成！\n",
      "編號news0010已爬蟲完成！\n",
      "編號news0011已爬蟲完成！\n",
      "編號news0012已爬蟲完成！\n",
      "編號news0013已爬蟲完成！\n",
      "編號news0014已爬蟲完成！\n",
      "編號news0015已爬蟲完成！\n",
      "編號news0016已爬蟲完成！\n",
      "編號news0017已爬蟲完成！\n",
      "編號news0018已爬蟲完成！\n",
      "編號news0019已爬蟲完成！\n",
      "編號news0020已爬蟲完成！\n",
      "編號news0021已爬蟲完成！\n",
      "編號news0022已爬蟲完成！\n",
      "編號news0023已爬蟲完成！\n",
      "編號news0024已爬蟲完成！\n",
      "編號news0025已爬蟲完成！\n",
      "編號news0026已爬蟲完成！\n",
      "編號news0027已爬蟲完成！\n",
      "編號news0028已爬蟲完成！\n",
      "編號news0029已爬蟲完成！\n",
      "編號news0030已爬蟲完成！\n",
      "==========爬蟲執行完畢！==========\n"
     ]
    }
   ],
   "source": [
    "# 導入jieba段詞\n",
    "import jieba\n",
    "# 爬網站內文設定\n",
    "for n in range(len(newsUrls)):\n",
    "    response = requests.get(\n",
    "        str(newsUrls[n]))\n",
    "    soupStory = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # print(soup.prettify())  #輸出排版後的HTML內容 除錯用\n",
    "\n",
    "    story = soupStory.find_all('p')\n",
    "    story\n",
    "    storyStr = ''\n",
    "    p_count = 0\n",
    "    for i in range(len(story)):\n",
    "        word = soupStory.find_all('p')[i].string\n",
    "        if word and p_count < 7:   #還沒想到如何對不同新聞的不同標籤下找內文\n",
    "            # print(word)\n",
    "            if '為達最佳瀏覽效果' in word:\n",
    "                continue\n",
    "            if '爆' in word:\n",
    "                continue\n",
    "            if '請繼續往下閱讀...' in word:\n",
    "                continue\n",
    "            if word and word !='':\n",
    "                storyStr += soupStory.find_all('p')[i].string\n",
    "                p_count += 1\n",
    "            storyStr += '\\n'\n",
    "    storyStr = storyStr.replace(\" \", \"\")\n",
    "    storyStr = storyStr.lstrip()\n",
    "    jiebaword = ' '.join(jieba.lcut(storyStr))\n",
    "    idpath = \"news_story/news\" + str(n+1).rjust(4,'0') + \".txt\"\n",
    "    with open(idpath, 'w') as f:\n",
    "        f.write(jiebaword)\n",
    "        if len(jiebaword) == 0:\n",
    "            f.write('此篇新聞內文為影片！此篇新聞內文為影片！此篇新聞內文為影片！')\n",
    "        print(\"編號news\" + str(n+1).rjust(4,'0') + '已爬蟲完成！')\n",
    "print('==========爬蟲執行完畢！==========')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('stable')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51a5e28b5cae5e55e931eb722c5118c5a01931b239abc598736cc2c6d4cddc07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
